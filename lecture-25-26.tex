\Lecture{Jayalal Sarma}{September, 16 2015}{25}{Introduction to Polynomials}{Punit Khanna}{$\beta$}{Ramya C}

\section{Introduction}
So far, we have studied various problems through the framework of group theory. Now we'll move on to the new theme of polynomials, while borrowing structural ideas from what we've studied so far.\\
We give broad definitions of the problems on polynomials that we're interested in:\\

\begin{problem}
Given a system of $k$ $n$-variate polynomial equtions
\begin{center}
$f_1(x_1, x_2, \ldots, x_n) = 0$\\
$f_2(x_1, x_2, \ldots, x_n) = 0$\\
\hspace{20 mm}\vdots \\
$f_k (x_1, x_2, \ldots, x_n) = 0$
\end{center}
find assignments to the variables $x_1, x_2, \ldots, x_n$ such that they satisfy the system of $k$ equations.
%\begin{equation}
%	\label{polynomial_equation}
%f_i (x_1, x_2, \ldots, x_n) = 0
%\end{equation}
\end{problem}

\begin{problem}[\textsc{Polynomial Factorization}]
Given a polynomial, enumerate its irreducible factors.
\end{problem}

A polynomial $f$ is said to be {\em identically zero} if all its cofficients are 0. 

\begin{problem}[\textsc{Polynomial Identity Testing}]
Given a polynomial $f$, test if it is identically 0.
\end{problem}

\section{Solving multivariate polynomial equations}
We begin by giving a formal definition of polynomials:
\begin{definition} (Polynomial)
A polynomial (in $n$ variables) is an expression which is the sum of terms of the form $ \alpha\prod\limits_{i=1}^{n} x_i^{\beta_i}$. Each such term is called a \textbf{monomial}. $\alpha$ is the \textbf{coefficient} while $\prod\limits_{i=1}^{n} x_i^{\beta_i}$ is the \textbf{power product}.
\end{definition}


Let us consider linear polynomials. That is, polynomials whose degree is 1. An intuitive way to represent the coefficients of $k$ linear polynomials in $n$ variables is to consider a matrix $A_{k \times n}$. Let $X$ be the column vector $(x_1, x_2, \ldots, x_n)$. Then we can view any 
system of $k$ linear equations as the matrix $A_{k \times n}$ acting on the left of the column vector $X$. A system of linear equations can therefore be written as the following:.\\
\[ AX = B \]
Here the column vector $B$ will be 0.\\
To solve this system of equations, we carry out the familiar process called \textbf{Gaussian Elimination}. We use a sequence of elementary row operations on $A_{k \times n}$ to transform it into a upper triangular matrix. Also observe that some of the rows of the matrix thus obtained can be completely zero. However solving the equations is now easy. When we apply this matrix on the column vector, the first non-zero row from the bottom is in only one variable ($x_n$); the equation resulting from the row just above has two variables (one of which $x_n$, has already been fixed), and so on.\\

Inorder to solve a system of linear equations we transfor the matrix representing the system of linear equation by applying elementary row and column operations. Why is this process correct ? Why does it not change the linear equations in the system ? To see this we will need a few basic definitions. We can view the rows of the matrix  $A_{k \times n}$ as vectors of the form $(a_1,\ldots,a_n)$. 
%such that the lower half of the matrix as divided by the diagonal coomes to consist entirely of elements equal to 0.

\begin{definition} (Row Space)
The set of all the vectors generated by elementary operations carried on the row vectors is called row space.
\end{definition}

\begin{definition} (Column Space)
The set of all the vectors generated by elementary operations carried on the column vectors is called column space.
\end{definition}

\begin{example}\label{row_space_example}
If $R_1$ and $R_2$ are row vectors that belong to some row space, then $R_1 + \alpha R_2$ also belongs to the same row space (similarly for columns as well), $\alpha$ being a constant.
\end{example}

\begin{definition} (Span) 
Let $R$ be a set of vectors. Then the set of all linear combinations of the vectors in $R$ is called {\em Span[$R$]}.
\end{definition}

\begin{definition} (Kernel of a linear transformation.)
Let $V$ be a set of vectors. Let $\phi$ be a linear transformation from $V$ to $V$. The set of all vectors in $V$ that are mapped to $0$ under $\phi$ is called Kernel of the linear transformation $\phi$. (Observe that this analogous to the definition of kernel that we already studied as part of Group Homomorphisms).
\end{definition}

We can see that the elementary row operations we did as part of Gaussian Elimination do not change the row space. In addition, the Kernel of the vector set under some operation, remains unchanged. This therefore establishes the correctness of the transformation done as part of Gaussian elimination.\\
At this point we ask ourselves a question. How is the kernel related to the row space (or column space, for that matter)?\\


We can see that for $B$ to be a zero vector, each row vector in $A$ must a dot product with $X$ equal to 0. Only then will the kernel contain $X$.\\


So far we were interested in linear combinations of polynomials. NOw we will look at combinations of polynomials by using polynomials as coefficients.


We now define a combination of the polynomials $f_1,f_2,...,f_k$ as follows:\\
\[
I = \left\lbrace\sum\limits_{i=1}^{k} g_{i}f_{i}\mid \text{~}g_{i}'s \text{~are polynomials}\right\rbrace
\]
%Here we have generalized the concept of row space. In the example~\ref{row_space_example}, we had used constants, but here we can use any polynomial $g_i$.\\
We can see that the following statement holds true:
\begin{lemma}
Let $f_1(x_1, x_2, \ldots, x_n),f_2(x_1, x_2, \ldots, x_n),\ldots,
f_k (x_1, x_2, \ldots, x_n)$ be polynomials.
\begin{equation}
 \forall i\in[k], (a_1,a_2,a_3.....,a_n)\text{~is a solution to~}f_i=0 \Leftrightarrow \forall h \in I \text{,~}h(a_1,a_2,a_3,....,a_n)=0
\end{equation}
\end{lemma}
\begin{proof} 
Since $h\in I$ we have $h=\sum\limits_{i=1}^{k} g_{i}f_{i}$. If every $f_i$ is zero on some value, $h$ will be 0 as well. The reverse direction is true because $I$ actually contains every $f_i$; if every $h$ is zero on some value, every $f_i$ is too.
\end{proof}

Therefore we can see that to calculate a common zero for all $f_i$s, we need a common zero for all polynomials in $I$. Our aim is to therefore obtain polynomials $l_1,l_2,l_3.....,l_{k'}$, such that $I = \{\sum\limits_{i=1}^{k'} g_{i}l_{i}| \text{~}g_{i}s \text{~are polynomials}\}$ is easy to solve. Here the $l_i$s form a \textbf{basis} for $I$. In particular, we call these a Grobner basis (we will study more about this in subsequent lectures).\\

\section{An Introduction to Algebraic Structures}
We have already studies what groups are in detail. Now we define the following:
\begin{definition} (Ring)
A ring is a set of elements that forms an Abelian group under addition, and respects the properties of associativity and closure under multiplication.
\end{definition}
\begin{definition} (Division Ring)
A ring that contains a multiplicative inverse is called a division ring.
\end{definition}
\begin{definition} (Commutative Ring)
A ring that is commutative under multiplication is called a commutative ring.
\end{definition}
\begin{example}
$\mathbb{Z}$ is a commutative ring.
\end{example}
\begin{definition} (Field)
A field (denoted by $\mathbb{F}$) is a ring that has a multiplicative inverse, and is commutative under multiplication. (A field is therefore an Abelian group under both addition and multiplication)
\end{definition}

\begin{example}
A polynomial in $n$ variables with integer coefficients (denoted by $\mathbb{Z}[x_1,x_2,...,x_n]$) is a commutative ring. Indeed, any polynomial is a commutative ring provided its coefficients are drawn from a commutative ring.
\end{example}
\begin{example}
$\mathbb{Z}_7$ is a finite ring.\\
\end{example}
\begin{definition} (Zero Divisor)
A ring $R$ is said to have a zero divisor if $\exists a,b\in R \neq 0$ in the ring such that $ab = 0$.
\end{definition}
\begin{definition} (Integral Domain)
An integral domain is a ring with no zero divisor.
\end{definition}
\begin{example}
$\mathbb{Z}$ is an integral domain while $\mathbb{Z}_4$ is not. Any $\mathbb{Z}_p$ where $p$ is prime is a field.
\end{example}
\begin{theorem}
Any finite integral domain is a field.
\end{theorem}

\Lecture{Jayalal Sarma}{September, 18 2015}{26}{Geometrical Representation of Polynomials}{Punit Khanna}{$\beta$}{Ramya C}

In the previous lecture, we looked at polynomials and defined some basic algebraic structures. Now, we will attempt to formalize some of our ideas as well as introduce the notions of \textbf{ideal} and \textbf{variety}.\\

\begin{notation}
A set of polynomials in $n$ variables whose coefficients are in $\mathbb{F}$ is denoted by $\mathbb{F}[x_1,x_2,x_3,\dots,x_n]$.
\end{notation}

%Similarly, a set of polynomials in $n$ variables which draw their coefficients from a ring $R$ is denoted by $R[x_1,x_2,x_3,.....,x_n]$.\\

\begin{observation}
$\mathbb{F}[x_1,x_2,\ldots,x_n]$ forms a ring. Let us denote $R:=\mathbb{F}[x_1,x_2,\ldots,x_n]$.
\end{observation}

The polynomial $5x_1^2x_2 + 7x_1x_2^2 + 8x_1x_2$ is part of $\mathbb{Z}[x_1,x_2]$, which can also be written as $(\mathbb{Z}[x_1])[x_2]$. (Here $\mathbb{Z}[x_1]$ is the ring from which coefficients are drawn).\\

We can the definition of $I$ from the previous lecture as follows:
\begin{equation}
I = \left\lbrace \sum\limits_{i=1}^{k} g_{i}f_{i}\mid \forall i, g_i \in R\right\rbrace
\end{equation}
We can make the following observations about $I$:
\begin{itemize}
\item $I \subseteq R$
\item $I$ follows the closure property
\item $I$ has inverse
\item $I$ contains the additive identity (0)
\item $I$ is a subgroup of $R$ w.r.t. addition
\item $I$ is a subring of $R$ w.r.t. both addition and multiplication\\
\end{itemize}


\begin{observation}
$RI \subseteq I$ 
\end{observation}
\begin{proof}
Let $h \in RI$\\
$\implies h=rh'$ such that $r \in R, h' \in I$\\
$\implies h=r\sum\limits_{i=1}^{k} g_{i}f_{i}$\\
$\implies h=\sum\limits_{i=1}^{k} (rg_{i})f_{i}$\\
$\implies h \in R$
\end{proof}

Observe that the above property is a stronger closure property than the usual closure property.


Having obtained this characterisation of $I$ as defined above, we give the following definition:
\begin{definition} (Ideal)
A set $I\subseteq R$ is said to be an ideal of the ring $R$ if :
\begin{itemize}
\item $I$ is a subgroup of $R$ w.r.t. addition
\item $RI \subseteq I$
\end{itemize}
\end{definition}

\section{Geometry of polynomials}
Let $\mathbb{F}$ be a field. We define $\mathbb{F}^n$ as 
\[
\mathbb{F}^n = \{(a_1,a_2,....,a_n) \mid a_i \in \mathbb{F}\}
\]

We define Variety ($\mathbb{V}$) as the following.

Let $f\in\mathbb{F}[x_1,\dots,x_n]$. The variety of $f$ denoted by $\mathbb{V}(f)$ is defined as

\begin{equation}
\mathbb{V}(f) = \{(a_1,a_2,...,a_n) | f(a_1,a_2,...,a_n)=0\}
\end{equation}

We also take variety to apply to a set of polynomials. 

Let $S \subseteq R$. Then the variety of $S$ denoted by $\mathbb{V}(S)$ is defined as 
\begin{equation}
\mathbb{V}(S) = \{(a_1,a_2,...,a_n) | \forall f \in S, f(a_1,a_2,...,a_n)=0\}
\end{equation}

We can see that the Variety is analogous to the root of a function, or rather the set of points at which multiple functions evaluate to 0.\\

Given a set of polynomials $S$ we defined the set of points that are common zeros of the polynomials $S$. Now given a set of points let us define the set of polynomials that vanish on those points. 



Let $V \in \mathbb{F}^n$:
\begin{equation}
\mathbb{I}(V) = \{h \in \mathbb{F}[x_1,x_2,...,x_n] | \forall (a_1,a_2,...,a_n) \in V, h(a_1,a_2,...,a_n) = 0 \}
\end{equation}


Having defined the above, we now pose the following questions:
\begin{itemize}
\item What is the relation between $V$ and $\mathbb{V}(\mathbb{I}(V))$?
\item What is the relation between $I$ and $\mathbb{I}(\mathbb{V}(I))$?
\end{itemize}









