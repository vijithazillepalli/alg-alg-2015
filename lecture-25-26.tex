\Lecture{Jayalal Sarma}{September, 16 2015}{25}{Introduction to Polynomials}{Punit Khanna}{$\alpha$}{Ramya C}

\section{Introduction}
So far, we have studied various problems through the framework of group theory. Now we'll move on to the new theme of polynomials, while borrowing structural ideas from what we've studied so far.\\
We give broad definitions of the problems on polynomials that we're interested in:\\

\begin{problem}
Given a system of $k$ n-variate polynomials, find satisfying assignments to the variables i.e. solutions to a system of $k$ equations of the form:
\begin{equation}
	\label{polynomial_equation}
f_i (x_1, x_2, \ldots, x_n) = 0
\end{equation}
\end{problem}

\begin{problem}[\textsc{Polynomial Factorization}]
Given a polynomial, enumerate its irreducible factors.
\end{problem}

\begin{problem}[\textsc{Polynomial Identity Testing}]
Given a polynomial, check if its identity is 0.
\end{problem}

\section{Solving multivariate polynomial equations}
We begin by giving a formal definition of polynomials:
\begin{definition} (Polynomial)
A polynomial (in $n$ variables) is an expression which is the sum of terms of the form $ \alpha\prod\limits_{i=1}^{n} x_i^{\beta_i}$. Each such term is called a \textbf{monomial}. $\alpha$ is the \textbf{coefficient} while $\prod\limits_{i=1}^{n} x_i^{\beta_i}$ is the \textbf{power product}.
\end{definition}
An intuitive way to represent these polynomials is to have a matrix $A_{k \times n}$ (where $k$ is the number of polynomials, and $n$ the number of variables) act on a column vector containing the variables (say $X$) from the left. A system of equations, where each equation is as outlined in~\ref{polynomial_equation}, can therefore be written as the following:.\\
\[
AX = B
\]
Here the column vector $B$ will be 0.\\
To solve this system of equations, we carry out a process called \textbf{Gaussian Elimination}. We use a sequence of elementary row operations on $A_{k \times n}$ such that the lower half of the matrix as divided by the diagonal coomes to consist entirely of elements equal to 0. Solving the equations is now easy. When we apply this matrix on the column vector, the final equation (resulting from the bottom row) is in only one variable ($x_n$); the equation resulting from the row just above has two variables (one of which $x_n$, has already been fixed), and so on.\\
We define a few terms:\\

\begin{definition} (Row Space)
The set of all the vectors generated by elementary operations carried on the row vectors is called row space.
\end{definition}

\begin{definition} (Column Space)
The set of all the vectors generated by elementary operations carried on the column vectors is called column space.
\end{definition}

\begin{example}\label{row_space_example}
If $R_1$ and $R_2$ are row vectors that belong to some row space, then $R_1 + \alpha R_2$ also belongs to the same row space (similarly for columns as well), $\alpha$ being a constant.
\end{example}

\begin{definition} (Span) 
The set of all linear combinations of the vectors contained within some vector set $R$ is called Span[$R$].
\end{definition}

\begin{definition} (Kernel)
The set of vectors that get mapped to $0$ under some linear transformation $\phi$ is called Kernel($\phi$) (Thi is analogous to the definition of kernel that we already studied as part of Group Homomorphism).
\end{definition}

We can see that the elementary row operations we did as part of Gaussian Elimination do not change the row space. In addition, the Kernel of the vector set under some operation, remains unchanged. This therefore establishes the correctness of the transformation done as part of Gaussian elimination.\\
At this point we ask ourselves a question. How is the kernel related to the row space (or column space, for that matter)?\\
We can see that for $B$ to be a zero vector, each row vector in $A$ must a dot product with $X$ equal to 0. Only then will the kernel contain $X$.\\

We now define a combination of the polynomials $f_1,f_2...f_k$ as follows:\\
\[
I = \{\sum\limits_{i=1}^{k} g_{i}f_{i}| \text{~}g_{i}s \text{~are polynomials}\}
\]
Here we have generalized the concept of row space. In the example~\ref{row_space_example}, we had used constants, but here we can use any polynomial $g_i$.\\
We can see that the following holds true:
\begin{equation}
 \forall i, (a_1,a_2,a_3.....,a_k)\text{~is a solution to~}f_i=0 \Leftrightarrow \forall h \in I \text{,~}h(a_1,a_2,a_3,....,a_k)=0
\end{equation}
The forward direction is easy to prove. Since every $g_{i}f_{i}$ term in every $h$ contains an $f_i$, if every $f_i$ is zero on some value, every $h$ will be as well. The reverse direction is true because $I$ actually contains every $f_i$; if every $h$ is zero on some value, every $f_i$ is too.\\
Therefore we can see that to calculate a common zero for all $f_i$s, we need a common zero for all polynomials in $I$. Our aim is to therefore obtain polynomials $l_1,l_2,l_3.....,l_{k'}$, such that $I = \{\sum\limits_{i=1}^{k} g_{i}f_{i}| \text{~}g_{i}s \text{~are polynomials}\}$ is easy to solve. Here the $l_i$s form a \textbf{basis} for $I$. In particular, we call these a Grobner basis (we'll study more about this in subsequent lectures).\\

\section{An Introduction to Algebraic Structures}
We have already studies what groups are in detail. Now we define the following:
\begin{definition} (Ring)
A ring is a set of elements that forms an Abelian group under addition, and respects the properties of associativity and closure under multiplication.
\end{definition}
\begin{definition} (Division Ring)
A ring that contains a multiplicative inverse is called a division ring.
\end{definition}
\begin{definition} (Commutative Ring)
A ring that is commutative under multiplication is called a commutative ring.
\end{definition}
\begin{definition} (Field)
A field is a ring that has a multiplicative inverse, and is commutative under multiplication. (A field is therefore an Abelian group under both addition and multiplication)
\end{definition}
\begin{example}
$\mathbb{Z}$ is a commutative ring.
\end{example}
\begin{example}
A polynomial in $n$ variables with integer coefficients (denoted by $\mathbb{Z}[x_1,x_2,...,x_n]$) is a commutative ring. Indeed, any polynomial is a commutative ring provided its coefficients are drawn from a commutative ring.
\end{example}
\begin{example}
$\mathbb{Z}_7$ is a finite ring.\\
\end{example}
\begin{definition} (Zero Divisor)
A ring is said to have a zero divisor if $\exists a,b \neq 0$ in the ring such that $ab = 0$.
\end{definition}
\begin{definition} (Integral Domain)
An integral domain is a ring with no zero divisor.
\end{definition}
\begin{example}
$\mathbb{Z}$ is an integral domain while $\mathbb{Z}_4$ is not. Any $\mathbb{Z}_p$ where $p$ is prime is a field.
\end{example}
\begin{theorem}
Any finite integral domain is a field.
\end{theorem}

\Lecture{Jayalal Sarma}{September, 18 2015}{26}{Geometrical Representation of Polynomials}{Punit Khanna}{$\alpha$}{Ramya C}

In the previous lecture, we looked at polynomials and defined some basic algebraic structures. Now, we'll attempt to formalize some of our ideas as well as introduce the notion of \textbf{ideal} and \textbf{variety}.\\

We'll first specify some notations.\\
Field is referred to by $\mathbb{F}$.\\
A set of polynomials in $n$ variables which draw their coefficients from $\mathbb{F}$ is denoted by $\mathbb{F}[x_1,x_2,x_3,.....,x_n]$.\\
Similarly, a set of polynomials in $n$ variables which draw their coefficients from a ring $R$ is denoted by $R[x_1,x_2,x_3,.....,x_n]$.\\
Note: $\mathbb{F}[x_1,x_2,x_3,.....,x_n]$ is actually a ring as it lacks a multiplicative inverse. Thus we will also use $R$ to denote it from now on.\\

We can also decompose these structures. For instance, the polynomial $5x_1^2x_2 + 7x_1x_2^2 + 8x_1x_2$ is part of $\mathbb{Z}[x_1,x_2]$, which can also be written as $(\mathbb{Z}[x_1])[x_2]$. (Here $\mathbb{Z}[x_1]$ is the ring from which coefficients are drawn).\\

We can the definition of $I$ from the previous lecture as follows:
\begin{equation}
I = \{\sum\limits_{i=1}^{k} g_{i}f_{i}| \forall i, g_i \in R\}
\end{equation}
We can make the following observations about $I$:
\begin{itemize}
\item $I \subseteq R$
\item $I$ follows the closure property
\item $I$ has inverse
\item $I$ contains the additive identity (0)
\item $I$ is a subgroup of $R$ w.r.t. addition
\item $I$ is a subring of $R$ w.r.t. both addition and multiplication\\
\end{itemize}

\begin{observation}
$RI \subseteq I$ (stronger closure property)
\end{observation}
\begin{proof}
Let $h \in RI$\\
$\implies h=rh'$ such that $r \in R, h' \in I$\\
$\implies h=r\sum\limits_{i=1}^{k} g_{i}f_{i}$\\
$\implies h=\sum\limits_{i=1}^{k} (rg_{i})f_{i}$\\
$\implies h \in R$
\end{proof}

Having obtained this characterisation of $I$ as defined above, we give the following definition:
\begin{definition} (Ideal)
A set $I$ is said to be an ideal of the ring $R$ if:
\begin{itemize}
\item $I$ is a subgroup of $R$ w.r.t. addition
\item $RI \subseteq I$
\end{itemize}
\end{definition}

\section{Geometry of polynomials}
Let $\mathbb{F}$ be a field containing the elements $x_1,x_2,...,x_n$.\\
We define $\mathbb{F}^n$ as 
\[
\mathbb{F}^n = \{(a_1,a_2,....,a_n) | a_i \in \mathbb{F}\}
\]

We define Variety ($\mathbb{V}$) as the following:
\begin{equation}
\mathbb{V}(f) = \{(a_1,a_2,...,a_n) | f \in R, f(a_1,a_2,...,a_n)=0\}
\end{equation}

We also take variety to apply to a set of polynomials, say $S \subseteq R$:
\begin{equation}
\mathbb{V}(S) = \{(a_1,a_2,...,a_n) | \forall f \in S, f(a_1,a_2,...,a_n)=0\}
\end{equation}

We can see that the Variety is analogous to the root of a function, or rather the set of points at which multiple functions evaluate to 0.\\
We also define the following, given $V \in \mathbb{F}^n$:
\begin{equation}
\mathbb{I}(V) = \{h \in \{x_1,x_2,...,x_n\} | \forall (a_1,a_2,...,a_n) \in V, h(a_1,a_2,...,a_n) = 0 \}
\end{equation}
We can see that $\mathbb{I}(V)$ is analogous to the set of functions which evaluate to zero at the points defined at $V$.\\

Having defined the above, we now pose the following questions:
\begin{itemize}
\item What is the relation between $V$ and $\mathbb{V}(\mathbb{I}(V))$?
\item What is the relation between $I$ and $\mathbb{I}(\mathbb{V}(I))$?
\end{itemize}









